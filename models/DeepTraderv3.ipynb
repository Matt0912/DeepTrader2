{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.1.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "## V3 CREATED \n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please ensure you have installed TensorFlow correctly')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Matt\\anaconda3\\envs\\gputest\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_data = pd.read_csv('result.csv', header=None)\n",
    "trader_ranks = full_data[full_data.columns[0]]\n",
    "data = full_data.drop(full_data.columns[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Matt\\anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1   2           3           4           5           6           7   \\\n",
      "0   3.9750   0  146.000000 -149.000000   74.000000    0.100000  149.000000   \n",
      "1   4.3125   0   81.000000 -122.000000   61.000000    0.000000  122.000000   \n",
      "2   4.3625   0  118.000000 -118.000000   59.000000    0.000000  118.000000   \n",
      "3   5.1625   1  110.000000   17.222222  100.388889  100.388889   91.777778   \n",
      "4   7.6625   0   93.509259    0.509259   94.254630   94.254630   94.000000   \n",
      "5   8.2875   0   86.000000    1.167695   93.925412   93.925412   93.341564   \n",
      "6  10.8875   0   57.000000    0.806173   91.403086   91.403086   91.000000   \n",
      "7  11.1125   0   89.687449    1.806173   90.903086   90.903086   90.000000   \n",
      "8  11.3875   0   87.747188    0.370782   89.185391   89.247188   89.000000   \n",
      "9  11.4000   0   87.747188    0.370782   89.185391   89.185391   89.000000   \n",
      "\n",
      "           8       9   10  11          12        13          14  \n",
      "0    0.200000  3.9750   1   4    0.300000  0.400000  149.100000  \n",
      "1    0.000000  0.3375   1   3  149.000000  0.000000  122.000000  \n",
      "2    0.000000  0.0500   1   3  135.500000  0.070450  118.000000  \n",
      "3  109.000000  0.8000   0   7  129.666667  0.089916  109.000000  \n",
      "4   94.509259  2.5000  -1  11  124.500000  0.109834   94.000000  \n",
      "5   94.509259  0.6250  -1  10  121.001783  0.121540   93.341564  \n",
      "6   91.806173  2.6000  -1  12  108.973505  0.209248   91.000000  \n",
      "7   91.806173  0.2250  -1  11  102.559425  0.256369   90.000000  \n",
      "8   89.370782  0.2750  -1   9   96.357761  0.307199   89.000000  \n",
      "9   89.370782  0.0125  -1   7   91.748750  0.344530   89.000000  \n",
      "0    71\n",
      "1    29\n",
      "2     9\n",
      "3    25\n",
      "4    26\n",
      "5    22\n",
      "6    11\n",
      "7    20\n",
      "8    24\n",
      "9     2\n",
      "Name: 0, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[9][0] = 3.9750000000000227\n",
    "data[9] = data[9].astype(float)\n",
    "print(data.head(10))\n",
    "print(trader_ranks.head(10))\n",
    "type(data[9][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalise data and convert ranks in to weights (using reciprocal function)\n",
    "def reciprocal(x):\n",
    "    return float(1/x)\n",
    "\n",
    "normalised_data=(data-data.min())/(data.max()-data.min())\n",
    "trader_ranks_frame = trader_ranks.to_frame()\n",
    "trader_weights = trader_ranks_frame.applymap(reciprocal)\n",
    "\n",
    "# print(normalised_data.head(10))\n",
    "# print(trader_weights.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0.014085\n",
      "1          0.034483\n",
      "2          0.111111\n",
      "3          0.040000\n",
      "4          0.038462\n",
      "             ...   \n",
      "9757413    0.029412\n",
      "9757414    0.100000\n",
      "9757415    1.000000\n",
      "9757416    0.035714\n",
      "9757417    0.142857\n",
      "Name: 0, Length: 9757418, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "\n",
    "validation_split = 0.1\n",
    "train, validation = train_test_split(normalised_data, test_size=validation_split, shuffle=False)\n",
    "train_trader_weights, val_trader_weights = train_test_split(trader_weights, test_size=validation_split, shuffle=False)\n",
    "\n",
    "print(train_trader_weights[0])\n",
    "\n",
    "train_x = train.loc[:, :13]\n",
    "train_y = train.loc[:, 14]\n",
    "val_x = validation.loc[:, :13]\n",
    "val_y = validation.loc[:, 14]\n",
    "\n",
    "x_train = train_x.to_numpy()\n",
    "y_train = train_y.to_numpy()\n",
    "x_val = val_x.to_numpy()\n",
    "y_val = val_y.to_numpy()\n",
    "\n",
    "x_train = x_train[:,:, newaxis]\n",
    "x_val = x_val[:,:, newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 9757418 samples, validate on 1084158 samples\n",
      "Epoch 1/10\n",
      "9757418/9757418 [==============================] - 99s 10us/sample - loss: 0.0048 - val_loss: 0.0089\n",
      "Epoch 2/10\n",
      "9757418/9757418 [==============================] - 63s 7us/sample - loss: 8.2065e-04 - val_loss: 0.0054\n",
      "Epoch 3/10\n",
      "9757418/9757418 [==============================] - 65s 7us/sample - loss: 5.3885e-04 - val_loss: 0.0043\n",
      "Epoch 4/10\n",
      "9757418/9757418 [==============================] - 65s 7us/sample - loss: 4.1605e-04 - val_loss: 0.0034\n",
      "Epoch 5/10\n",
      "9757418/9757418 [==============================] - 63s 6us/sample - loss: 3.2098e-04 - val_loss: 0.0029\n",
      "Epoch 6/10\n",
      "9757418/9757418 [==============================] - 63s 6us/sample - loss: 2.8322e-04 - val_loss: 0.0028\n",
      "Epoch 7/10\n",
      "9757418/9757418 [==============================] - 64s 7us/sample - loss: 2.6653e-04 - val_loss: 0.0027\n",
      "Epoch 8/10\n",
      "9757418/9757418 [==============================] - 63s 7us/sample - loss: 2.5829e-04 - val_loss: 0.0026\n",
      "Epoch 9/10\n",
      "9757418/9757418 [==============================] - 63s 6us/sample - loss: 2.5271e-04 - val_loss: 0.0026\n",
      "Epoch 10/10\n",
      "9757418/9757418 [==============================] - 63s 6us/sample - loss: 2.4845e-04 - val_loss: 0.0025\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM#, CuDNNLSTM\n",
    "#from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(x_train.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(LSTM(6, input_shape=(x_train.shape[1:]), return_sequences=False))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=3e-5, decay=1e-6)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=1024, validation_data=(x_val, y_val))# sample_weight=train_trader_weights[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79744691 0.01408451 0.991     ]\n",
      " [0.59205073 0.03448276 0.72      ]\n",
      " [0.55702937 0.11111111 0.68      ]\n",
      " [0.5270167  0.04       0.59      ]\n",
      " [0.4786323  0.03846154 0.44      ]\n",
      " [0.45188734 0.04545455 0.43341564]\n",
      " [0.38297749 0.09090909 0.41      ]\n",
      " [0.41279837 0.05       0.4       ]\n",
      " [0.39703849 0.04166667 0.39      ]\n",
      " [0.39672169 0.5        0.39      ]\n",
      " [0.54863393 0.01694915 0.54      ]\n",
      " [0.40542799 0.03030303 0.38      ]\n",
      " [0.4817467  0.16666667 0.51      ]\n",
      " [0.58172858 0.02857143 0.55      ]\n",
      " [0.41249114 0.03703704 0.41      ]\n",
      " [0.54915988 0.1        0.508     ]\n",
      " [0.37271646 0.33333333 0.38      ]\n",
      " [0.43959472 0.02083333 0.48      ]\n",
      " [0.5483703  0.02325581 0.54      ]\n",
      " [0.52695447 0.05882353 0.56      ]\n",
      " [0.50578606 0.01785714 0.56      ]\n",
      " [0.59119183 0.06666667 0.57      ]\n",
      " [0.61326605 0.25       0.61      ]\n",
      " [0.61468726 0.01470588 0.61      ]\n",
      " [0.66511613 0.14285714 0.65      ]\n",
      " [0.5211128  0.02325581 0.58333333]\n",
      " [0.64425725 0.01515152 0.67      ]\n",
      " [0.64315003 0.08333333 0.63777778]\n",
      " [0.58158141 0.01923077 0.62      ]\n",
      " [0.64436883 1.         0.64      ]\n",
      " [0.67443722 0.0625     0.68      ]\n",
      " [0.56504458 0.03030303 0.62      ]\n",
      " [0.64524299 0.03333333 0.69      ]\n",
      " [0.60702002 0.1        0.6       ]\n",
      " [0.60383755 0.04       0.6       ]\n",
      " [0.6634351  0.03571429 0.59      ]\n",
      " [0.5892024  0.125      0.58      ]\n",
      " [0.60591012 0.06666667 0.59      ]\n",
      " [0.6080541  0.04545455 0.61      ]\n",
      " [0.51171845 0.05882353 0.52      ]\n",
      " [0.52817065 0.04347826 0.55      ]\n",
      " [0.63174397 0.07142857 0.64      ]\n",
      " [0.57701129 0.2        0.56      ]\n",
      " [0.51892436 0.08333333 0.54      ]\n",
      " [0.53215474 0.5        0.53      ]\n",
      " [0.51826888 0.05       0.51      ]\n",
      " [0.4336873  0.05555556 0.43      ]\n",
      " [0.50448477 0.02173913 0.53      ]\n",
      " [0.58125293 0.01754386 0.55      ]\n",
      " [0.51764166 0.03125    0.56      ]\n",
      " [0.45405111 0.01428571 0.4       ]\n",
      " [0.63524228 0.02564103 0.66      ]\n",
      " [0.7152825  0.07692308 0.68      ]\n",
      " [0.70082849 0.14285714 0.69      ]\n",
      " [0.62005842 0.05882353 0.63      ]\n",
      " [0.62832707 0.5        0.7       ]\n",
      " [0.55506867 0.09090909 0.54      ]\n",
      " [0.56744152 0.25       0.57444444]\n",
      " [0.61655998 0.04545455 0.61254321]\n",
      " [0.60633934 0.03030303 0.61      ]\n",
      " [0.63364381 0.03333333 0.68      ]\n",
      " [0.63516182 0.02857143 0.65      ]\n",
      " [0.55355257 0.33333333 0.54      ]\n",
      " [0.51565772 0.01449275 0.49592593]\n",
      " [0.58179146 0.01886792 0.49      ]\n",
      " [0.47622883 0.07142857 0.48      ]\n",
      " [0.4851878  0.01612903 0.48888889]\n",
      " [0.43058255 0.04       0.46      ]\n",
      " [0.47393498 1.         0.46      ]\n",
      " [0.51499707 0.025      0.5       ]\n",
      " [0.47736165 0.0212766  0.49972222]\n",
      " [0.40113631 0.0625     0.43      ]\n",
      " [0.55416417 0.16666667 0.51      ]\n",
      " [0.42616591 0.03703704 0.41      ]\n",
      " [0.48924252 0.125      0.5       ]\n",
      " [0.42135423 0.01666667 0.42      ]\n",
      " [0.47262993 0.02272727 0.5       ]\n",
      " [0.51224214 0.03571429 0.51      ]\n",
      " [0.4445022  0.06666667 0.49      ]\n",
      " [0.43993834 0.2        0.46      ]\n",
      " [0.46496198 0.01724138 0.46      ]\n",
      " [0.49398294 0.25       0.49      ]\n",
      " [0.42253694 0.11111111 0.41      ]\n",
      " [0.44730309 1.         0.44      ]\n",
      " [0.50413227 0.02702703 0.45      ]\n",
      " [0.45632473 0.07142857 0.44      ]\n",
      " [0.55139267 0.01538462 0.48      ]\n",
      " [0.42763767 0.03703704 0.43      ]\n",
      " [0.55145079 0.125      0.51      ]\n",
      " [0.42693508 0.05555556 0.42      ]\n",
      " [0.38648415 0.02173913 0.41      ]\n",
      " [0.42104715 0.33333333 0.42333333]\n",
      " [0.43229884 0.04347826 0.43333333]\n",
      " [0.47227994 0.02857143 0.47      ]\n",
      " [0.56036556 0.025      0.48      ]\n",
      " [0.47855046 0.5        0.51      ]\n",
      " [0.57818776 0.16666667 0.52      ]\n",
      " [0.42799914 0.02222222 0.47      ]\n",
      " [0.50484121 0.0625     0.52      ]\n",
      " [0.40375888 0.04545455 0.41868313]]\n",
      "[[0.56554896 0.125      0.59      ]\n",
      " [0.58305794 0.2        0.61      ]\n",
      " [0.58474928 0.02272727 0.53      ]\n",
      " [0.61717159 0.5        0.75      ]\n",
      " [0.76085931 0.04761905 0.76      ]\n",
      " [0.53991538 0.02439024 0.55      ]\n",
      " [0.51017684 0.02       0.47      ]\n",
      " [0.41920409 0.09090909 0.4       ]\n",
      " [0.50134826 0.04       0.49      ]\n",
      " [0.4251889  0.07142857 0.37      ]\n",
      " [0.40254784 0.5        0.38      ]\n",
      " [0.5802511  0.05882353 0.66      ]\n",
      " [0.5238651  1.         0.68      ]\n",
      " [0.59520811 0.1        0.64      ]\n",
      " [0.58774179 0.02272727 0.61      ]\n",
      " [0.57958037 0.06666667 0.58      ]\n",
      " [0.53078318 0.08333333 0.53      ]\n",
      " [0.63828748 0.25       0.7       ]\n",
      " [0.63359332 0.05555556 0.73      ]\n",
      " [0.58428019 0.05882353 0.62      ]\n",
      " [0.56368321 1.         0.52      ]\n",
      " [0.55163521 0.04166667 0.59      ]\n",
      " [0.53545517 0.14285714 0.51      ]\n",
      " [0.5331586  0.04166667 0.52      ]\n",
      " [0.74550116 0.33333333 0.74      ]\n",
      " [0.47152606 0.02222222 0.39      ]\n",
      " [0.63985664 0.03703704 0.79      ]\n",
      " [0.7453922  0.05882353 0.76      ]\n",
      " [0.54387897 0.02777778 0.44      ]\n",
      " [0.82130343 0.04347826 0.8       ]\n",
      " [0.51491004 0.02272727 0.41      ]\n",
      " [0.69039673 0.04761905 0.67      ]\n",
      " [0.58837849 0.03125    0.53      ]\n",
      " [0.50003469 0.02857143 0.59      ]\n",
      " [0.58154327 0.07142857 0.56      ]\n",
      " [0.44003424 0.5        0.44      ]\n",
      " [0.40074694 1.         0.41      ]\n",
      " [0.42482838 0.02941176 0.39      ]\n",
      " [0.61356831 0.05       0.58      ]\n",
      " [0.68858737 0.25       0.84      ]\n",
      " [0.56396955 0.33333333 0.51      ]\n",
      " [0.5207299  0.2        0.54      ]\n",
      " [0.59410542 0.08333333 0.69      ]\n",
      " [0.59474319 0.16666667 0.57      ]\n",
      " [0.70285672 0.14285714 0.71      ]\n",
      " [0.76940566 0.02564103 0.72      ]\n",
      " [0.48605356 0.05555556 0.45      ]\n",
      " [0.46246779 0.01960784 0.4       ]\n",
      " [0.58666748 0.07692308 0.76      ]\n",
      " [0.44315201 0.11111111 0.37      ]\n",
      " [0.41631815 0.03030303 0.34      ]\n",
      " [0.38366005 0.04166667 0.48      ]\n",
      " [0.21520089 0.16666667 0.23      ]\n",
      " [0.64761764 1.         0.63      ]\n",
      " [0.61808395 0.5        0.61      ]\n",
      " [0.59613228 0.05882353 0.56      ]\n",
      " [0.47271907 0.07142857 0.42      ]\n",
      " [0.3855072  0.05       0.29      ]\n",
      " [0.67997193 0.11111111 0.7       ]\n",
      " [0.73793226 0.33333333 0.78      ]\n",
      " [0.55090946 0.25       0.53      ]\n",
      " [0.29237914 0.02941176 0.19      ]\n",
      " [0.59807479 0.03030303 0.6       ]\n",
      " [0.28793147 0.25       0.33      ]\n",
      " [0.59220576 1.         0.56      ]\n",
      " [0.62165588 0.06666667 0.82      ]\n",
      " [0.44764891 0.5        0.38      ]\n",
      " [0.59065646 0.06666667 0.45      ]\n",
      " [0.57864851 0.04166667 0.51      ]\n",
      " [0.65654135 0.08333333 0.66      ]\n",
      " [0.61950296 0.05555556 0.68      ]\n",
      " [0.5917303  0.04761905 0.55      ]\n",
      " [0.4721587  0.09090909 0.4       ]\n",
      " [0.6371333  0.07142857 0.7       ]\n",
      " [0.57125419 0.04545455 0.54      ]\n",
      " [0.38159326 0.14285714 0.35      ]\n",
      " [0.57544488 0.01666667 0.56      ]\n",
      " [0.55030018 0.01587302 0.54      ]\n",
      " [0.55281526 0.01818182 0.54      ]\n",
      " [0.43147728 0.02941176 0.45      ]\n",
      " [0.47084975 0.16666667 0.49      ]\n",
      " [0.48691162 0.03703704 0.37      ]\n",
      " [0.45445231 0.125      0.39      ]\n",
      " [0.47093979 0.01538462 0.49      ]\n",
      " [0.46883124 0.03571429 0.52      ]\n",
      " [0.48359442 0.07692308 0.49      ]\n",
      " [0.54920542 0.2        0.47      ]\n",
      " [0.52826136 0.015625   0.52      ]\n",
      " [0.47871333 0.05       0.52      ]\n",
      " [0.46109214 0.5        0.51      ]\n",
      " [0.40440056 0.05882353 0.42      ]\n",
      " [0.56917268 0.03333333 0.54      ]\n",
      " [0.58407909 0.02777778 0.59      ]\n",
      " [0.55776006 0.04166667 0.6       ]\n",
      " [0.62917715 0.01886792 0.62      ]\n",
      " [0.66106838 0.14285714 0.68      ]\n",
      " [0.60770702 0.25       0.65      ]\n",
      " [0.58125234 0.04761905 0.63      ]\n",
      " [0.60630494 0.01639344 0.6       ]\n",
      " [0.41304964 0.02325581 0.4       ]]\n"
     ]
    }
   ],
   "source": [
    "## Previous loss: loss: 2.1564e-04 - val_loss: 0.0016\n",
    "## 512, 1e-4, no dropout, no weights = loss: 8.8952e-04 - val_loss: 9.5390e-04\n",
    "## 1024, 3e-5, no dropout, no weights = loss: 0.0015 - val_loss: 0.0020 (best performing!)\n",
    "## 1024, 3e-5, no dropout, weights = loss: 2.4845e-04 - val_loss: 0.0025\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "train_values = x_train[:100]\n",
    "print(np.c_[model.predict(train_values), train_trader_weights[:100], y_train[:100]])\n",
    "test_values = x_val[:100]\n",
    "print(np.c_[model.predict(test_values), val_trader_weights[:100], y_val[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('DeepTraderv3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084158/1084158 [==============================] - 151s 139us/sample - loss: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0019653245837587055"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('DeepTraderv3_no_weights.h5')\n",
    "\n",
    "new_model.evaluate(x_val,  y_val, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12594831]\n",
      "[0.12594831]\n",
      "1084158/1084158 [==============================] - 146s 135us/sample - loss: 0.0020\n",
      "[1.]\n",
      "[1.]\n",
      "1084158/1084158 [==============================] - 145s 134us/sample - loss: 0.0036\n",
      "[0.15]\n",
      "[0.15]\n",
      "1084158/1084158 [==============================] - 144s 133us/sample - loss: 0.0070\n",
      "[0.14363636]\n",
      "[0.14363636]\n",
      "1084158/1084158 [==============================] - 143s 132us/sample - loss: 0.0025\n",
      "[0.16]\n",
      "[0.16]\n",
      "1084158/1084158 [==============================] - 149s 137us/sample - loss: 0.0034\n",
      "[0.12207792]\n",
      "[0.12207792]\n",
      "1084158/1084158 [==============================] - 132s 121us/sample - loss: 0.0025\n",
      "[0.50666667]\n",
      "[0.50666667]\n",
      "1084158/1084158 [==============================] - 125s 115us/sample - loss: 0.0075\n",
      "[0.11]\n",
      "[0.11]\n",
      "1084158/1084158 [==============================] - 153s 141us/sample - loss: 0.0020\n",
      "[0.12604341]\n",
      "[0.12604341]\n",
      "1084158/1084158 [==============================] - 150s 139us/sample - loss: 0.0020\n",
      "[0.]\n",
      "[0.]\n",
      "1084158/1084158 [==============================] - 150s 139us/sample - loss: 0.0020\n",
      "[0.359375]\n",
      "[0.359375]\n",
      "1084158/1084158 [==============================] - 153s 141us/sample - loss: 0.0020\n",
      "[0.58036385]\n",
      "[0.58036385]\n",
      "1084158/1084158 [==============================] - 153s 141us/sample - loss: 0.0020\n",
      "[0.0693102]\n",
      "[0.0693102]\n",
      "1084158/1084158 [==============================] - 152s 141us/sample - loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "## Shuffle each column systematically to see the effect on loss\n",
    "for i in range(0, x_val.shape[1]):\n",
    "    modx_val = np.copy(x_val)\n",
    "\n",
    "    # Shuffle first column of modified x_val\n",
    "    np.random.shuffle(modx_val[:,i])\n",
    "    print(modx_val[0,i])\n",
    "    #np.random.shuffle(modx_val[:,i])\n",
    "    print(modx_val[0,i])\n",
    "    new_model.evaluate(modx_val,  y_val, verbose=1) \n",
    "\n",
    "#### Ideal loss: 6.5164e-04 --- WEIGHTED TRADERS\n",
    "\n",
    "## time = loss: 7.4031e-04, 7.3929e-04\n",
    "## ask/bid flag = loss: 0.0032\n",
    "## customer order = loss: 0.0043\n",
    "## spread - loss: 0.0024\n",
    "## midprice - loss: 0.0086\n",
    "## microprice - loss: 0.0018\n",
    "## best bid - loss: 0.0054\n",
    "## best ask - loss: 7.3251e-04, 7.3453e-04\n",
    "## time since last trade - loss: 6.5241e-04, 6.5216e-04\n",
    "## LOB imbalance - 6.9200e-04, 6.9134e-04\n",
    "## total quotes - 6.5322e-04, 6.5318e-04\n",
    "## p* - loss: 6.5659e-04, 6.5649e-04\n",
    "## Smith's alpha - loss: 6.5867e-04, 6.5885e-04\n",
    "\n",
    "#### Ideal loss: 0.0020 --- UNWEIGHTED TRADERS (all same)\n",
    "\n",
    "## time = loss: 0.0020, 0.0020\n",
    "## ask/bid flag = loss: 0.0036, 0.0036\n",
    "## customer order = loss: 0.0070, 0.0070\n",
    "## spread - loss: 0.0025, 0.0025\n",
    "## midprice - loss: 0.0034, \n",
    "## microprice - loss: 0.0025\n",
    "## best bid - loss: 0.0075\n",
    "## best ask - loss: 0.0020\n",
    "## time since last trade - loss: 0.0020\n",
    "## LOB imbalance - 0.0020\n",
    "## total quotes - 0.0020\n",
    "## p* - loss: 0.0020\n",
    "## Smith's alpha - loss: 0.0020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "      <td>1.084158e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.132783e+02</td>\n",
       "      <td>4.998728e-01</td>\n",
       "      <td>1.005552e+02</td>\n",
       "      <td>8.221978e+00</td>\n",
       "      <td>1.017929e+02</td>\n",
       "      <td>1.017555e+02</td>\n",
       "      <td>9.785417e+01</td>\n",
       "      <td>1.060761e+02</td>\n",
       "      <td>1.487492e+00</td>\n",
       "      <td>-4.642593e-01</td>\n",
       "      <td>2.973506e+01</td>\n",
       "      <td>1.014007e+02</td>\n",
       "      <td>1.087417e-01</td>\n",
       "      <td>1.016784e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.748708e+02</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2.222623e+01</td>\n",
       "      <td>1.791955e+01</td>\n",
       "      <td>1.301607e+01</td>\n",
       "      <td>1.322090e+01</td>\n",
       "      <td>1.132019e+01</td>\n",
       "      <td>1.926697e+01</td>\n",
       "      <td>1.373372e+00</td>\n",
       "      <td>4.994087e-01</td>\n",
       "      <td>8.904971e+00</td>\n",
       "      <td>9.897357e+00</td>\n",
       "      <td>4.088805e-02</td>\n",
       "      <td>1.079134e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>-1.500000e+02</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.250000e-02</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.609125e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.731626e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>9.100000e+01</td>\n",
       "      <td>9.800000e+01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>9.596383e+01</td>\n",
       "      <td>8.284752e-02</td>\n",
       "      <td>9.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.172125e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>3.592593e+00</td>\n",
       "      <td>1.012992e+02</td>\n",
       "      <td>1.012843e+02</td>\n",
       "      <td>9.800000e+01</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>1.100000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>1.015867e+02</td>\n",
       "      <td>9.943374e-02</td>\n",
       "      <td>1.014566e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.735500e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.140000e+02</td>\n",
       "      <td>1.166667e+01</td>\n",
       "      <td>1.080000e+02</td>\n",
       "      <td>1.080000e+02</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>1.130000e+02</td>\n",
       "      <td>2.050000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.600000e+01</td>\n",
       "      <td>1.072586e+02</td>\n",
       "      <td>1.256213e-01</td>\n",
       "      <td>1.090000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>9.500000e+02</td>\n",
       "      <td>5.750000e+02</td>\n",
       "      <td>7.700000e+02</td>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>2.996250e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.500000e+01</td>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>8.724786e-01</td>\n",
       "      <td>1.500000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1             2             3             4             5   \\\n",
       "count  1.084158e+07  1.084158e+07  1.084158e+07  1.084158e+07  1.084158e+07   \n",
       "mean   3.132783e+02  4.998728e-01  1.005552e+02  8.221978e+00  1.017929e+02   \n",
       "std    1.748708e+02  5.000000e-01  2.222623e+01  1.791955e+01  1.301607e+01   \n",
       "min    2.500000e-01  0.000000e+00  5.000000e+01 -1.500000e+02  2.500000e+01   \n",
       "25%    1.609125e+02  0.000000e+00  8.731626e+01  1.000000e+00  9.500000e+01   \n",
       "50%    3.172125e+02  0.000000e+00  1.010000e+02  3.592593e+00  1.012992e+02   \n",
       "75%    4.735500e+02  1.000000e+00  1.140000e+02  1.166667e+01  1.080000e+02   \n",
       "max    6.000000e+02  1.000000e+00  1.500000e+02  9.500000e+02  5.750000e+02   \n",
       "\n",
       "                 6             7             8             9             10  \\\n",
       "count  1.084158e+07  1.084158e+07  1.084158e+07  1.084158e+07  1.084158e+07   \n",
       "mean   1.017555e+02  9.785417e+01  1.060761e+02  1.487492e+00 -4.642593e-01   \n",
       "std    1.322090e+01  1.132019e+01  1.926697e+01  1.373372e+00  4.994087e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  1.250000e-02 -1.000000e+00   \n",
       "25%    9.500000e+01  9.100000e+01  9.800000e+01  5.000000e-01 -1.000000e+00   \n",
       "50%    1.012843e+02  9.800000e+01  1.050000e+02  1.100000e+00  0.000000e+00   \n",
       "75%    1.080000e+02  1.050000e+02  1.130000e+02  2.050000e+00  0.000000e+00   \n",
       "max    7.700000e+02  1.500000e+02  1.000000e+03  2.996250e+01  1.000000e+00   \n",
       "\n",
       "                 11            12            13            14  \n",
       "count  1.084158e+07  1.084158e+07  1.084158e+07  1.084158e+07  \n",
       "mean   2.973506e+01  1.014007e+02  1.087417e-01  1.016784e+02  \n",
       "std    8.904971e+00  9.897357e+00  4.088805e-02  1.079134e+01  \n",
       "min    1.000000e+00  0.000000e+00  0.000000e+00  5.000000e+01  \n",
       "25%    2.300000e+01  9.596383e+01  8.284752e-02  9.400000e+01  \n",
       "50%    3.000000e+01  1.015867e+02  9.943374e-02  1.014566e+02  \n",
       "75%    3.600000e+01  1.072586e+02  1.256213e-01  1.090000e+02  \n",
       "max    6.500000e+01  1.500000e+02  8.724786e-01  1.500000e+02  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
